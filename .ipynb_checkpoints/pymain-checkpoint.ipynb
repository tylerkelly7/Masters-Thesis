{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14ed17d5-3441-42fa-9772-e9c6d22ad61f",
   "metadata": {},
   "source": [
    "##### Improving Prediction Accuracy of Sepsis using Natural Language Processing\n",
    "## Tyler Kelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd196b-2a8c-476f-8122-5da74fd724e6",
   "metadata": {},
   "source": [
    "# Set Up and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba4bbcd-afdf-4f3d-b47b-1f9216e5c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install datasets transformers pandas shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c39c3",
   "metadata": {},
   "source": [
    "## Part 0 Preprocessing (Pull Code from Author's ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5889d9",
   "metadata": {},
   "source": [
    "The following code is adapted from the github repository 'https://github.com/yuyinglu2000/Sepsis-Mortality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e5e92c",
   "metadata": {},
   "source": [
    "### Data Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30907322",
   "metadata": {},
   "source": [
    "Begin by creating bigquery search to get the 38 unique features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb3e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv('Data/data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0205a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.shape\n",
    "# Expect to get a dataframe 808188x38 (38 old columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "# regroup the race\n",
    "race_mapping = {\n",
    "    'WHITE': 'White',\n",
    "    'HISPANIC OR LATINO': 'Hispanic or Latin',\n",
    "    'BLACK/AFRICAN AMERICAN': 'Black or African American',\n",
    "    'BLACK/CARIBBEAN ISLAND': 'Black or African American',\n",
    "    'HISPANIC/LATINO - DOMINICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - CENTRAL AMERICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - GUATEMALAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - PUERTO RICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - SALVADORAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - HONDURAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - MEXICAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - CUBAN': 'Hispanic or Latin',\n",
    "    'HISPANIC/LATINO - COLUMBIAN': 'Hispanic or Latin',\n",
    "    'BLACK/CAPE VERDEAN': 'Black or African American',\n",
    "    'BLACK/AFRICAN': 'Black or African American',\n",
    "    'SOUTH AMERICAN': 'Hispanic or Latin',\n",
    "    'WHITE - BRAZILIAN': 'Hispanic or Latin',\n",
    "    'WHITE - OTHER EUROPEAN': 'White',\n",
    "    'WHITE - RUSSIAN': 'White',\n",
    "    'WHITE - EASTERN EUROPEAN': 'White',\n",
    "    'ASIAN': 'Others race',\n",
    "    'ASIAN - SOUTH EAST ASIAN': 'Others race',\n",
    "    'ASIAN - CHINESE': 'Others race',\n",
    "    'ASIAN - ASIAN INDIAN': 'Others race',\n",
    "    'ASIAN - KOREAN': 'Others race',\n",
    "    'AMERICAN INDIAN/ALASKA NATIVE': 'Others race',\n",
    "    'NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER': 'Others race',\n",
    "    'MULTIPLE RACE/ETHNICITY': 'Others race',\n",
    "    'PORTUGUESE': 'Others race',\n",
    "    'UNKNOWN': 'Others race',\n",
    "    'OTHER': 'Others race',\n",
    "    'UNABLE TO OBTAIN': 'Others race',\n",
    "    'PATIENT DECLINED TO ANSWER': 'Others race'\n",
    "}\n",
    "\n",
    "df_raw['race'] = df_raw['race'].map(race_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# df = ... (your DataFrame)\n",
    "\n",
    "# Define a mapping for antibiotics to their respective groups\n",
    "antibiotic_mapping = {\n",
    "    'Gentamicin Sulfate': 'Aminoglycoside',\n",
    "    'Tobramycin Sulfate': 'Aminoglycoside',\n",
    "    'Streptomycin Sulfate': 'Aminoglycoside',\n",
    "    'Neomycin Sulfate': 'Aminoglycoside',\n",
    "    'Neomycin/Polymyxin B Sulfate': 'Aminoglycoside',\n",
    "    'Meropenem': 'Carbapenem',\n",
    "    'Meropenem Graded Challenge': 'Carbapenem',\n",
    "    'Vancomycin': 'Glycopeptide',\n",
    "    'Vancomycin Oral Liquid': 'Glycopeptide',\n",
    "    'Vancomycin Antibiotic Lock': 'Glycopeptide',\n",
    "    'Vancomycin Enema': 'Glycopeptide',\n",
    "    'Vancomycin Intrathecal': 'Glycopeptide',\n",
    "    'Vancomycin Ora': 'Glycopeptide',\n",
    "    'Linezolid': 'Oxazolidinone',\n",
    "    'Linezolid Suspension': 'Oxazolidinone',\n",
    "    'Penicillin G Benzathine': 'Penicillin',\n",
    "    'Penicillin G Potassium': 'Penicillin',\n",
    "    'Penicillin V Potassium': 'Penicillin',\n",
    "    'Sulfameth/Trimethoprim': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim DS': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim SS': 'Sulfonamide',\n",
    "    'Sulfamethoxazole-Trimethoprim': 'Sulfonamide',\n",
    "    'Sulfameth/Trimethoprim Suspension': 'Sulfonamide',\n",
    "    'Tetracycline': 'Tetracycline',\n",
    "    'Tetracycline HCl': 'Tetracycline'\n",
    "    # Add more mappings as needed\n",
    "}\n",
    "\n",
    "# Applying the mapping to the 'antibiotic' column\n",
    "df_raw['antibiotic'] = df_raw['antibiotic'].map(antibiotic_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56745eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "df_raw['antibiotic'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9cc603",
   "metadata": {},
   "source": [
    "### Get Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419f807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC\n",
    "df_encoded = pd.get_dummies(df_raw, columns=df_raw.select_dtypes(include=['object']).columns)\n",
    "df_dropped = df_encoded.dropna()\n",
    "df_dropped.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check empty values *for tetracycline* ###\n",
    "#AC\n",
    "empty_values = df_dropped['antibiotic_Tetracycline'].isnull().any()\n",
    "empty_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c6d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c38a1c0",
   "metadata": {},
   "source": [
    "After applying get_dummy_variables there is now 53 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ebb55",
   "metadata": {},
   "source": [
    "### Drop Duplications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AC with minor edits\n",
    "\n",
    "duplicated_rows_mask = df_dropped['subject_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated rows\n",
    "duplicated_rows = df_dropped[duplicated_rows_mask]\n",
    "new_data  = df_dropped.drop_duplicates()\n",
    "duplicated_rows_mask = new_data['subject_id'].duplicated(keep=False)\n",
    "\n",
    "# Extract the duplicated rows\n",
    "duplicated_rows = new_data[duplicated_rows_mask]\n",
    "# Separate out columns based on data types\n",
    "int_float_cols = new_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "uint8_cols = new_data.select_dtypes(include=['uint8']).columns\n",
    "\n",
    "# Sort dataframe\n",
    "# For int and float columns: sort in descending order so that larger values come first\n",
    "df_raw = new_data.sort_values(by=list(int_float_cols), ascending=False)\n",
    "\n",
    "# For uint8 columns: sort in descending order so that 1 comes before 0\n",
    "df_raw = df_raw.sort_values(by=list(uint8_cols), ascending=False)\n",
    "\n",
    "# Drop duplicates based on subject_id, keeping the first (which are the desired rows after sorting)\n",
    "df_reduced = df_raw.drop_duplicates(subset='subject_id', keep='first')\n",
    "\n",
    "# Reset index if needed\n",
    "df_reduced = df_reduced.reset_index(drop=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "df_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d9a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82be4d9",
   "metadata": {},
   "source": [
    "After reducing the dataframe we get the 6401 patients reported in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec042c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c051a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.to_csv(\"data_ready_to_merge.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5df87b",
   "metadata": {},
   "source": [
    "## Part 1 Upload data_ready_to_merge.csv to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905776e4",
   "metadata": {},
   "source": [
    "Upload the dataset to bigquery and merge the dataset to radiology and discharge notes. Save the downloaded file to downloads (or find a way to save it directly to my BIOST 2021 Thesis / Main ) as\n",
    "'data_full_notes.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edac5ba",
   "metadata": {},
   "source": [
    "Place SQL code chunks below (if time write them into the script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6943900",
   "metadata": {},
   "source": [
    "# READ THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987127df",
   "metadata": {},
   "source": [
    "data_full_notes_old.csv uses an old outdated dataset but I haven't figured out the correct sql to get the correct dataset at 6401.\n",
    "\n",
    "For now, I will use `df_old` when using the outdated dataset and `df` when I get the new corrected one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de455f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Old/data_full_notes_old.csv\")\n",
    "# change after fixing sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be76a458-f382-4e39-93c8-fabdf0b024b3",
   "metadata": {},
   "source": [
    "## Part 2 Truncate Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013c98a8-9450-4fb4-9cd0-e3ae8d1adf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import re\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83765b2e-1ba0-4f72-94e6-0060b027568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Define cleaning function - Clean individual note text ===\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Normalize whitespace\n",
    "    text = re.sub(r'_+', '', text)    # Remove underlines\n",
    "    text = re.sub(r'[^\\w\\s.,:;!?()\\-\\n]', '', text)  # Remove junk, keep clinical symbols\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012ffaf-a180-45ac-b82f-8eda85aeca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 2: Function to process one group. Process a group into (subject_id, note_type, combined_notes) ===\n",
    "def process_group(record):\n",
    "    subject_id = record['subject_id']\n",
    "    note_type_1 = record['note_type_1'] #this column identifies addendums and base notes to just be 'radiology' notes\n",
    "    texts = record['text']\n",
    "    cleaned_notes = [clean_text(text) for text in texts]\n",
    "    combined_notes = \" \".join(cleaned_notes)\n",
    "    return {\n",
    "        'subject_id': subject_id,\n",
    "        'note_type_1': note_type_1,\n",
    "        'combined_notes': combined_notes\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893d23a-1fe0-48c2-b711-19aac68c1848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 3: Group the notes. Load and group your data ===\n",
    "# Replace this with your actual loading logic / dataframe\n",
    "# Data loaded above as `df`\n",
    "grouped_df = (\n",
    "    df.groupby(['subject_id', 'note_type_1'])['text']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "records = grouped_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef15737-8d7c-411c-b99f-8313cbf065d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303994, 58)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605c2269-543d-4012-89fa-e4e3f910979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 4: Parallel processing with joblib ===\n",
    "num_cores = multiprocessing.cpu_count() - 1\n",
    "\n",
    "processed = Parallel(n_jobs=num_cores)(\n",
    "    delayed(process_group)(record) for record in records\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510f691-b265-4c3c-8151-619dcdd69e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 5: Create DataFrame and save to csv ===\n",
    "nlp_long_df = pd.DataFrame(processed).sort_values(by=['subject_id', 'note_type_1'])\n",
    "\n",
    "nlp_long_df.to_csv(\"Data/Old/data_trunc_notes_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db47d90-108d-4afa-a844-81be9b9fd8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 6: Pivot to wide format for multiple columns ===\n",
    "# Convert note_type to columns like 'Radiology_notes', etc.\n",
    "nlp_wide_df = nlp_long_df.pivot(\n",
    "    index='subject_id',\n",
    "    columns='note_type_1',\n",
    "    values='combined_notes'\n",
    ").reset_index()\n",
    "\n",
    "nlp_wide_df.columns.name = None # Remove category label\n",
    "\n",
    "# Rename columns to make clear\n",
    "nlp_wide_df = nlp_wide_df.rename(columns={\n",
    "    'radiology': 'Radiology_notes',\n",
    "    'discharge': 'Discharge_summary_notes'\n",
    "})\n",
    "\n",
    "nlp_wide_df = nlp_wide_df.fillna(\"\") #fills NA columns with empty strings\n",
    "\n",
    "# Save\n",
    "nlp_wide_df.to_csv(\"Data/Old/data_trunc_notes_wide_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2be1ee-2fbb-400f-a13e-9d3bb14dff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 7: Combine Radiology and Discharge notes per subject_id ===\n",
    "nlp_combined_df = nlp_wide_df.copy()\n",
    "\n",
    "# Concatenate the two columns into one\n",
    "nlp_combined_df['combined_notes'] = (\n",
    "    nlp_combined_df['Radiology_notes'].str.strip() + \" \" +\n",
    "    nlp_combined_df['Discharge_summary_notes'].str.strip()\n",
    ").str.strip()\n",
    "\n",
    "# Combined DataFrame with just subject_id + combined text\n",
    "nlp_combined_notes_df = nlp_combined_df[['subject_id', 'combined_notes']]\n",
    "\n",
    "# Save\n",
    "nlp_combined_notes_df.to_csv(\"Data/Old/data_trunc_notes_combined_old.csv\", index=False) # change after fixing sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cdbd3d-3399-4449-80c6-648bd73160c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 8: Join nlp_wide_df and nlp_combined_notes_df to data_after_cleaning ===\n",
    "\n",
    "# === Step i: Load the original cleaned dataset, df_reduced ===\n",
    "# ie this is df_reduced\n",
    "# for now used data_clean until sql is fixed\n",
    "data_clean = pd.read_csv('Data/Old/data_after_cleaning.csv')\n",
    "\n",
    "# === Step ii: Merge df_reduced with nlp_wide_df ===\n",
    "# This adds the radiology and discharge notes as 2 new columns to df_reduced\n",
    "# use data_clean until df_reduced is finalized\n",
    "\n",
    "nlp_ready_df = data_clean.merge(\n",
    "    nlp_wide_df,\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# === Step iii: Merge with combined notes ===\n",
    "# This adds one new column of all notes combined together as a single note (per patient) to the nlp_ready_df above\n",
    "nlp_ready_df = nlp_ready_df.merge(\n",
    "    nlp_combined_notes_df,\n",
    "    on='subject_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Save\n",
    "nlp_ready_df.to_csv(\"Data/Old/data_nlp_ready_old.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094ddd80-f078-4f19-bafd-bb544b64d613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === Step 9: Check shape of dataframes ===\n",
    "nlp_wide_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f044b1-4c59-4cb6-b74c-32d9de161bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_combined_notes_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18979ae-ac9d-4c3e-aeb8-13bd72afdfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 49)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reduced.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43814b0-307d-42b1-b565-1316c6bb1821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5208, 52)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ready_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43007975-bac1-499e-bcd9-33695e1f7dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'subject_id', 'hospital_expire_flag', 'max_age',\n",
       "       'los_icu', 'first_hosp_stay', 'suspected_infection', 'sofa_score',\n",
       "       'sepsis3', 'avg_urineoutput', 'glucose_min', 'glucose_max',\n",
       "       'glucose_average', 'sodium_max', 'sodium_min', 'sodium_average',\n",
       "       'diabetes_without_cc', 'diabetes_with_cc', 'severe_liver_disease',\n",
       "       'aids', 'renal_disease', 'heart_rate_min', 'heart_rate_max',\n",
       "       'heart_rate_mean', 'sbp_min', 'sbp_max', 'sbp_mean', 'dbp_min',\n",
       "       'dbp_max', 'dbp_mean', 'resp_rate_min', 'resp_rate_max',\n",
       "       'resp_rate_mean', 'spo2_min', 'spo2_max', 'spo2_mean', 'coma',\n",
       "       'albumin', 'race_Black or African American', 'race_Hispanic or Latin',\n",
       "       'race_Others race', 'race_White', 'antibiotic_Vancomycin',\n",
       "       'antibiotic_Vancomycin Antibiotic Lock', 'antibiotic_Vancomycin Enema',\n",
       "       'antibiotic_Vancomycin Intrathecal',\n",
       "       'antibiotic_Vancomycin Oral Liquid', 'gender_F', 'gender_M',\n",
       "       'Discharge_summary_notes', 'Radiology_notes', 'combined_notes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_ready_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb06cff-23d0-4d94-ae50-e6dae3d3c9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5208 entries, 0 to 5207\n",
      "Data columns (total 52 columns):\n",
      " #   Column                                 Non-Null Count  Dtype  \n",
      "---  ------                                 --------------  -----  \n",
      " 0   Unnamed: 0                             5208 non-null   int64  \n",
      " 1   subject_id                             5208 non-null   int64  \n",
      " 2   hospital_expire_flag                   5208 non-null   int64  \n",
      " 3   max_age                                5208 non-null   int64  \n",
      " 4   los_icu                                5208 non-null   float64\n",
      " 5   first_hosp_stay                        5208 non-null   bool   \n",
      " 6   suspected_infection                    5208 non-null   int64  \n",
      " 7   sofa_score                             5208 non-null   int64  \n",
      " 8   sepsis3                                5208 non-null   bool   \n",
      " 9   avg_urineoutput                        5208 non-null   float64\n",
      " 10  glucose_min                            5208 non-null   float64\n",
      " 11  glucose_max                            5208 non-null   float64\n",
      " 12  glucose_average                        5208 non-null   float64\n",
      " 13  sodium_max                             5208 non-null   float64\n",
      " 14  sodium_min                             5208 non-null   float64\n",
      " 15  sodium_average                         5208 non-null   float64\n",
      " 16  diabetes_without_cc                    5208 non-null   int64  \n",
      " 17  diabetes_with_cc                       5208 non-null   int64  \n",
      " 18  severe_liver_disease                   5208 non-null   int64  \n",
      " 19  aids                                   5208 non-null   int64  \n",
      " 20  renal_disease                          5208 non-null   int64  \n",
      " 21  heart_rate_min                         5208 non-null   int64  \n",
      " 22  heart_rate_max                         5208 non-null   int64  \n",
      " 23  heart_rate_mean                        5208 non-null   float64\n",
      " 24  sbp_min                                5208 non-null   float64\n",
      " 25  sbp_max                                5208 non-null   float64\n",
      " 26  sbp_mean                               5208 non-null   float64\n",
      " 27  dbp_min                                5208 non-null   float64\n",
      " 28  dbp_max                                5208 non-null   float64\n",
      " 29  dbp_mean                               5208 non-null   float64\n",
      " 30  resp_rate_min                          5208 non-null   float64\n",
      " 31  resp_rate_max                          5208 non-null   float64\n",
      " 32  resp_rate_mean                         5208 non-null   float64\n",
      " 33  spo2_min                               5208 non-null   int64  \n",
      " 34  spo2_max                               5208 non-null   int64  \n",
      " 35  spo2_mean                              5208 non-null   float64\n",
      " 36  coma                                   5208 non-null   int64  \n",
      " 37  albumin                                5208 non-null   float64\n",
      " 38  race_Black or African American         5208 non-null   int64  \n",
      " 39  race_Hispanic or Latin                 5208 non-null   int64  \n",
      " 40  race_Others race                       5208 non-null   int64  \n",
      " 41  race_White                             5208 non-null   int64  \n",
      " 42  antibiotic_Vancomycin                  5208 non-null   int64  \n",
      " 43  antibiotic_Vancomycin Antibiotic Lock  5208 non-null   int64  \n",
      " 44  antibiotic_Vancomycin Enema            5208 non-null   int64  \n",
      " 45  antibiotic_Vancomycin Intrathecal      5208 non-null   int64  \n",
      " 46  antibiotic_Vancomycin Oral Liquid      5208 non-null   int64  \n",
      " 47  gender_F                               5208 non-null   int64  \n",
      " 48  gender_M                               5208 non-null   int64  \n",
      " 49  Discharge_summary_notes                5208 non-null   object \n",
      " 50  Radiology_notes                        5208 non-null   object \n",
      " 51  combined_notes                         5208 non-null   object \n",
      "dtypes: bool(2), float64(20), int64(27), object(3)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "nlp_ready_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bcaabe-ba2d-498d-9f79-1f2cad58c687",
   "metadata": {},
   "source": [
    "## Part 3 Create Note File for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f1a367-d882-4b2f-929b-4d2f7b35b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Write the 'Radiology_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_Radiology_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"Radiology_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3a5d89-c689-4fbd-9499-9e172a4aa1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the 'combined_notes' column to a text file, one line per document\n",
    "with open(\"Data/Old/W2V_old/w2v_combined_notes.txt\", \"w\", encoding=\"utf-8\") as f: # change after fixing sql\n",
    "    for line in nlp_ready_df[\"combined_notes\"]:\n",
    "        f.write(str(line).strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d0c99",
   "metadata": {},
   "source": [
    "## Part 4 Prepare Word2Vec - Proceed to main.rmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba3d2f9",
   "metadata": {},
   "source": [
    "## Part 5 Model Training - Proceed to Sepsis_Model_Training.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c224e",
   "metadata": {},
   "source": [
    "After completing / running / saving models in model training, upload them into the workspace in the following code chunks if necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519bc3e0-7f90-41dd-856d-6673a057c292",
   "metadata": {},
   "source": [
    "## Part 6 Create Dataset for Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb85a9e-e01b-47d2-8a6f-b1b9881fbc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "def write_single_note_clean(row, output_dir):\n",
    "    subject_id = str(row['subject_id'])\n",
    "    note_id = str(row['note_id'])\n",
    "    note_text = row['note_text'].strip()  # Clean text\n",
    "\n",
    "    subject_dir = os.path.join(output_dir, subject_id)\n",
    "    os.makedirs(subject_dir, exist_ok=True)\n",
    "\n",
    "    file_path = os.path.join(subject_dir, f\"{note_id}.txt\")\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(note_text)\n",
    "\n",
    "# Change for directories/output directories for combined/discharge notes\n",
    "def write_mimic_notes_parallel_for_bert(df, output_dir=\"pat_notes/rad_notes\", metadata_csv=\"rad_notes_metadata.csv\", max_workers=8):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    rows = df.to_dict(\"records\")\n",
    "\n",
    "    task = partial(write_single_note_clean, output_dir=output_dir)\n",
    "    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        list(executor.map(task, rows))\n",
    "\n",
    "    metadata_cols = ['subject_id', 'note_id', 'category', 'chartdate', 'charttime']\n",
    "    df[metadata_cols].to_csv(metadata_csv, index=False)\n",
    "\n",
    "    print(f\"âœ… Notes saved to: {output_dir}\")\n",
    "    print(f\"âœ… Metadata saved to: {metadata_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd40843-3906-409e-bb50-7cb99ef68e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def create_bert_dataset_from_notes(metadata_csv, notes_root_dir):\n",
    "    df = pd.read_csv(metadata_csv)\n",
    "\n",
    "    def load_text(row):\n",
    "        subject_id = str(row['subject_id'])\n",
    "        note_id = str(row['note_id'])\n",
    "        file_path = os.path.join(notes_root_dir, subject_id, f\"{note_id}.txt\")\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                return f.read().strip()\n",
    "        except FileNotFoundError:\n",
    "            return \"\"\n",
    "\n",
    "    df['note_text'] = df.apply(load_text, axis=1)\n",
    "    df = df[df['note_text'].str.strip() != \"\"]  # Remove blanks\n",
    "\n",
    "    dataset = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    print(f\"âœ… Dataset created with {len(dataset)} entries\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a4725-0a96-48e8-8739-1b41cff263e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_bert_dataset(dataset, model_name='emilyalsentzer/Bio_ClinicalBERT', text_column='note_text'):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "            example[text_column],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=512\n",
    "        )\n",
    "\n",
    "    print(\"ðŸ”„ Tokenizing dataset...\")\n",
    "    tokenized = dataset.map(tokenize_function, batched=True)\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b4f342-7970-4935-9000-09cb2868f19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tokenized_dataset(tokenized_dataset, output_path=\"Data/Old/BERT_old/clinical_bert_dataset\"):\n",
    "    tokenized_dataset.save_to_disk(output_path)\n",
    "    print(f\"âœ… Tokenized dataset saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db2c38c-4ba0-46b2-83e9-3c70477efe88",
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A process in the process pool was terminated abruptly while the future was running or pending.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1: Extract and save notes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m rad_trunc_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_trunc_notes.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m write_mimic_notes_parallel_for_bert(rad_trunc_df)\n",
      "Cell \u001b[1;32mIn[27], line 24\u001b[0m, in \u001b[0;36mwrite_mimic_notes_parallel_for_bert\u001b[1;34m(df, output_dir, metadata_csv, max_workers)\u001b[0m\n\u001b[0;32m     22\u001b[0m task \u001b[38;5;241m=\u001b[39m partial(write_single_note_clean, output_dir\u001b[38;5;241m=\u001b[39moutput_dir)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ProcessPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28mlist\u001b[39m(executor\u001b[38;5;241m.\u001b[39mmap(task, rows))\n\u001b[0;32m     26\u001b[0m metadata_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnote_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchartdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharttime\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     27\u001b[0m df[metadata_cols]\u001b[38;5;241m.\u001b[39mto_csv(metadata_csv, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\process.py:642\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[1;34m(iterable)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m    643\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    644\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:619\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[0;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 619\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop())\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:317\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fut\u001b[38;5;241m.\u001b[39mresult(timeout)\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A process in the process pool was terminated abruptly while the future was running or pending."
     ]
    }
   ],
   "source": [
    "# Step 1: Extract and save notes\n",
    "bert_rad = pd.read_csv(\"Data/Old/data_nlp_ready.csv\")\n",
    "write_mimic_notes_parallel_for_bert(bert_rad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db34057c-af04-4099-a0f8-5018bccf382d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Rebuild dataset\n",
    "rad_dataset = create_bert_dataset_from_notes(\"rad_notes_metadata.csv\", \"rad_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4b111b-58a9-419d-81bc-1b82ff2f09ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize with Clinical BERT\n",
    "tokenized_dataset = tokenize_bert_dataset(rad_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa815554-61eb-4575-8fd6-8172e3f11844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Save for later\n",
    "save_tokenized_dataset(tokenized_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
